---
title: "p8105_hw6_yx2954"
author: "Yiran Xu"
date: "2024-11-28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(modelr)
library(p8105.datasets)
library(mgcv)
```

# Problem 1 

Load data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Create and apply bootstrap generating function

```{r}
boot_sample <- function(df) {
  sample_frac(df, replace = TRUE)
}
```

We should also do a quick check to see if this is working.
```{r bootstrap_sample_check}
set.seed(456)

boot_sample(weather_df) |> 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm")
```

That looks about right. Then we generate 5,000 bootstrap samples
```{r}
boot_straps = tibble(strap_number = 1:5000) |>
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )
```

```{r}
bootstrap_results <- boot_straps |>
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin, data = df)),
    r2 = map_dbl(models, \(model) broom::glance(model)$r.squared),
    log_beta0_beta1 = map_dbl(models, \(model) {
      coefs <- broom::tidy(model)$estimate
      log(coefs[1] * coefs[2])
    })
  ) |>
  select(-strap_sample, -models) |>
  unnest(log_beta0_beta1) 
```

Plot the distribution of two sample estimates
```{r estimate_plot}
bootstrap_longer = bootstrap_results |>
  pivot_longer(
    cols = c(r2, log_beta0_beta1),
    names_to = "statistic",
    values_to = "value"
  )

estimate_plot = bootstrap_longer |>
  ggplot(aes(x = value)) +
  geom_density(alpha = 0.5, fill = "blue") +
  facet_wrap(~statistic, scales = "free", labeller = labeller(
    statistic = c(
      r2 = "R^2",
      log_beta0_beta1 = "log(β0 * β1)"
    )
  )) +
  labs(
    title = "Bootstrap Distributions",
    x = "Value",
    y = "Density"
  )

print(estimate_plot)
```

Get numeric values of sample estimates
```{r}
bootstrap_summary <- bootstrap_results %>%
  summarize(
    r2_mean = mean(r2),
    r2_sd = sd(r2),
    log_beta0_beta1_mean = mean(log_beta0_beta1),
    log_beta0_beta1_sd = sd(log_beta0_beta1)
  )
```

The plot shows that both sample estimates are nearly normally distributed. The sample mean of log(beta0 * beta1) is 2.013649 and standard error is 0.02389236. The maen of r2 is 0.9114209 and the standard error is 0.00849257.

Get CI for both sample estimates
```{r}
ci_r2 = quantile(bootstrap_results$r2, c(0.025, 0.975))
ci_log_beta0_beta1 <- quantile(bootstrap_results$log_beta0_beta1, c(0.025, 0.975))
```

The 95% CI of r2 is (0.8940791, 0.9271204), the 95% CI of log(beta0 * beta1) is (1.964630, 2.058959)

# Problem 2

Load data and data cleaning
```{r}
homicide_df <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") 

homicide_df = homicide_df |>
  mutate(city_state = paste(city, state, sep = ", "),
         status = if_else(disposition == "Closed by arrest", 1, 0)
           ) |>  
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black"),
    victim_age != "Unknown") |>
  mutate(victim_age = as.numeric(victim_age)) |>
  select(-city, - state, -disposition)
```

Logistic regression in Baltimore, MD

```{r}
baltimore_df = homicide_df |>
  filter(city_state == "Baltimore, MD")

baltimore_glm = glm(status ~ victim_age + victim_sex + victim_race,
                     data = baltimore_df,
                     family = binomial()) |>
  broom::tidy(exponentiate = TRUE, conf.int = TRUE)

baltimore_or = baltimore_glm |>
  filter(term == "victim_sexMale") |>
  select(term, estimate, conf.low, conf.high) |>
  knitr::kable(digits = 4)
```
The Odd ratio for solving homicides comparing male victims to female victims is 0.4255, with a 95% CI of (0.3242, 0.5576) and p value = 6.255119e-10. It suggests that the result is significant as the higher boundary is smaller than 1, and the probability of solving homicides of male victims is 42.6% as likely as that of female victims.

Run glm for each of the cities

```{r}
all_city_results = homicide_df |>
  group_by(city_state) |>
  nest() |>
  mutate(
    glm_model = map(data, ~ glm(status ~ victim_age + victim_sex + victim_race, data = ., family = binomial())),
    tidy_model = map(glm_model, ~ broom::tidy(.x, exponentiate = TRUE, conf.int = TRUE))
  ) |>
  unnest(tidy_model) |>
  filter(term == "victim_sexMale") %>%
  select(city_state, estimate, conf.low, conf.high) |>
  arrange(estimate) 
  
all_city_results |>
  knitr::kable(digits = 4)
```

Create a plot that shows the estimated ORs and CIs for each city.
```{r all_city_plot, fig.height=7, fig.width=10}
all_city_plot = all_city_results |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Adjusted Odds Ratios for Solving Homicides (Male vs Female)",
    x = "City",
    y = "Odds Ratio",
  ) +
  theme_minimal()

print(all_city_plot)
```

Comments: 
This plot displays the adjusted odds ratios (OR) for solving homicides for male victims compared to female victims across various cities, while controlling for other factors. An OR greater than 1 indicates a higher likelihood of resolution for male victims, while an OR less than 1 suggests the opposite. 

Among all cities included, Albuquerque, NM has the highest adjusted OR, indicating highest proportion of solving male homicides. But its 95% CI is very broad, suggesting the uncertainty of estimate caused by limited sample size. New York, NY has the lowest adjusted OR, suggesting the highest proportion of solving female homicides. 41 cities have an adjusted OR less than 1, while 6 cities have an adjusted OR larger than 1.

# Problem 3

Load and clean data
```{r}
bw_df = read_csv("https://p8105.com/data/birthweight.csv") |>
  mutate(
      babysex = recode_factor(babysex, `1` = "Male", `2` = "Female"), 
      frace = recode_factor(frace, `1` = "White", `2` = "Black", `3` = "Asian", 
                                   `4` = "Puerto Rican", `8` = "Other", `9` = "Unknown"),
      mrace = recode_factor(mrace, `1` = "White", `2` = "Black", `3` = "Asian", 
                                   `4` = "Puerto Rican", `8` = "Other"), 
      malform = recode_factor(malform, `0` = "Absent", `1` = "Present"), 
  )
    
sapply(bw_df, function(x) sum(is.na(x)))
summary(bw_df)
```

Then build my model:

Model Hypothesis: Based on empirical knowledge, birth weight may be related to pre-pregnant weight and gestational age in weeks. But as we don't know if this relationship is linear, I apply gam model instead of lm. The interaction between them are not considered. Here is my model:
```{r}
fit_hypothetical = gam(bwt ~ s(ppwt) + s(gaweeks), data = bw_df)
```

```{r}
bw_df = bw_df |>
  modelr::add_predictions(fit_hypothetical) |>
  modelr::add_residuals(fit_hypothetical)
```

```{r residuals_vs_gaweeks}
bw_df |> 
  modelr::add_residuals(fit_hypothetical) |> 
  ggplot(aes(x = gaweeks, y = resid)) + geom_point() + 
  labs(
    x = "Gestational Age", 
    y = "Residuals",
    title = "Residuals vs Gestational Age"
  ) +
  theme_minimal()
```
```{r residuals_vs_ppwt, fig.cap="residuals_vs_ppwt"}
bw_df |> 
  modelr::add_residuals(fit_hypothetical) |> 
  ggplot(aes(x = ppwt, y = resid)) +
  geom_point() +
  labs(
    x = "Pre-pregnancy Weight (ppwt)", 
    y = "Residuals",
    title = "Residuals vs Pre-pregnancy Weight"
  ) +
  theme_minimal()

```

```{r qqplot_residual}
residuals_hypothetical = residuals(fit_hypothetical)

qqnorm(residuals_hypothetical, main = "QQ Plot of Residuals")
qqline(residuals_hypothetical, col = "red", lwd = 2)
```
The qq plot shows that the residuals are approximately approach normal distribution. At the extreme ends, there are noticeable deviations from the red line. This suggests that the model might not fully capture the data's behavior for observations with extreme values.


## Then comparing my models to the other two models:

First generate train set and test set:
```{r}
set.seed(123)

cv_df = crossv_mc(bw_df, 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Then do the same using the other two models:
```{r}
cv_results = cv_df |>
  mutate(
    my_model = map(train, ~ gam(bwt ~ s(ppwt) + s(gaweeks), data = .x)),
    model_2 = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    model_3 = map(train, ~ lm(bwt ~ bhead * blength * babysex, data = .x))
  ) |>
  mutate(
    rmse_model_2 = map2_dbl(model_2, test, ~ rmse(model = .x, data = .y)),
    rmse_model_3 = map2_dbl(model_3, test, ~ rmse(model = .x, data = .y)),
    rmse_my_model = map2_dbl(my_model, test, ~ rmse(model = .x, data = .y))
  )
```

```{r model_comparison}
cv_results |>
  select(starts_with("rmse")) |>
  pivot_longer(
    cols = everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |>
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    title = "Cross-Validated RMSE Comparison",
    x = "Model",
    y = "RMSE"
  ) +
  theme_minimal()
```

From the figure above, my model is not as good as the other two models in terms of the value of RMSE. This might because that the two factors I chose are not the key factors that capture the major variance of the data. The interaction model is the best in terms of RMSE, but should also consider the model complexity by adding an additional factor.
